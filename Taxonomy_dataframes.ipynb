{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb2c982-83d9-4684-950a-6176aad71108",
   "metadata": {},
   "source": [
    "# Taxonomy tables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb829b0-7d3e-46b9-bc70-aaa21ac130a0",
   "metadata": {},
   "source": [
    "Make a table that has rows as taxa, columns as samples, and values as sqrt(normalised(sum(abundance))). \\\n",
    "Because the taxonomic classification is hierarcical we need to sum abundance from the rank and all lower rank. \\\n",
    "Let's say we had this: \n",
    "\n",
    "\n",
    "| Order     | Family      | Genus     | Species     | Abundance| \n",
    "| ----------| ------------|-----------| ------------|----------|   \n",
    "| Bryales   | Bryaceae    | Bryum     | capillare   | 10       |\n",
    "| Bryales   | Bryaceae    | Bryum     |             | 23       |\n",
    "| Bryales   | Bryaceae    |           |             | 45       |\n",
    "| Bryales   |             |           |             | 123      |  \n",
    "\n",
    "So the abundances we want to record for each rank are: \\\n",
    "Order   Bryales   123+45+23+10 \\\n",
    "Family  Bryaceae  45+23+10 \\\n",
    "Genus   Bryum     23+10 \\\n",
    "Species Bryum     10\n",
    "\n",
    "Do this for all named taxa at all ranks\n",
    "\n",
    "Also normalise data within each Superkingdom\n",
    "Normalisation - maximum absolute scaling. The maximum absolute scaling rescales each feature between -1 and 1 by dividing every observation by its maximum absolute value. We can apply the maximum absolute scaling in Pandas using the .max() and .abs() methods, as shown below.\n",
    "[link](https://www.geeksforgeeks.org/data-normalization-with-pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ebce0-7d15-4782-be36-123577af8a41",
   "metadata": {},
   "source": [
    "### Load the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45374f36-b746-4875-93ab-993a9a002613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import duckdb\n",
    "import numpy\n",
    "from duckdb import CatalogException, BinderException\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "from minio import Minio, S3Error\n",
    "from pathlib import Path\n",
    "\n",
    "creds = json.load(open(\"credentials.json\"))\n",
    "\n",
    "client = Minio(\n",
    "    \"10.4.1.4:9000\",\n",
    "    secure=False,\n",
    "    access_key=creds[\"accessKey\"],\n",
    "    secret_key=creds[\"secretKey\"],\n",
    ")\n",
    "\n",
    "\n",
    "def get_object(bucket_name, file_format, file_name, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"{bucket_name=} - {file_format=} - {file_name=}\")\n",
    "    try:\n",
    "        response = client.get_object(bucket_name, file_name)\n",
    "        buffer = io.BytesIO(response.read())\n",
    "    except S3Error:\n",
    "        raise\n",
    "    finally:\n",
    "        if file_format == \"parquet\":\n",
    "            df = pd.read_parquet(buffer, engine=\"pyarrow\")\n",
    "        elif file_format == \"csv\":\n",
    "            df = pd.read_csv(buffer)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown {file_format=}\")\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "        if verbose:\n",
    "            print(f\"Downloaded {file_name} into dataframe\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b1527-dcbc-4221-8932-9913e2418abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_VERSION = \"v2\"\n",
    "\n",
    "# MGF parquet tables to dataframes\n",
    "bucket_name = \"emo-bon-tables\"\n",
    "objects = client.list_objects(bucket_name, recursive=True)\n",
    "mgf_parquet_dfs = {}\n",
    "\n",
    "for obj in objects:\n",
    "    # print(f\"{obj.object_name=}\")\n",
    "    # Get v2 objects\n",
    "    if obj.object_name.split(\"/\")[0] == TABLE_VERSION:\n",
    "        name = obj.object_name.split(\".\")[-2]\n",
    "        # print(f\"Getting... {name}\")\n",
    "        df = get_object(bucket_name, \"parquet\", obj.object_name, verbose=False)\n",
    "        mgf_parquet_dfs[name] = df\n",
    "\n",
    "# Sample metadata\n",
    "# Get the latest Batch combined logsheets file\n",
    "# Remember we are downloading from MinIO\n",
    "# This breaks when new versions are available\n",
    "batch_file = \"Batch1and2_combined_logsheets_2024-09-11.csv\"\n",
    "sample_metadata = (\"emo-bon-metadata-tables\", \"csv\", batch_file)\n",
    "sample_metadata = get_object(*sample_metadata, verbose=False)\n",
    "\n",
    "# Observatory metadata - from the GoogleSheets\n",
    "observatory_metadata = (\n",
    "    \"emo-bon-metadata-tables\",\n",
    "    \"csv\",\n",
    "    \"Observatory_combined_logsheets_validated.csv\",\n",
    ")\n",
    "observatory_metadata = get_object(*observatory_metadata, verbose=False)\n",
    "\n",
    "# Into duckdb\n",
    "try:\n",
    "    duckdb.sql(\"DROP TABLE SAMPLE_METADATA\")\n",
    "    duckdb.sql(\"DROP TABLE OBS_METADATA\")\n",
    "    for table_name in mgf_parquet_dfs:\n",
    "        cmd = f\"DROP TABLE {table_name}\"\n",
    "        duckdb.sql(cmd)\n",
    "except CatalogException:\n",
    "    pass\n",
    "duckdb.sql(\"CREATE TABLE SAMPLE_METADATA AS SELECT * FROM sample_metadata\")\n",
    "duckdb.sql(\"SELECT COUNT(*) FROM SAMPLE_METADATA\")\n",
    "duckdb.sql(\"CREATE TABLE OBS_METADATA AS SELECT * FROM observatory_metadata\")\n",
    "duckdb.sql(\"SELECT COUNT(*) FROM OBS_METADATA\")\n",
    "for table_name in mgf_parquet_dfs:\n",
    "    df = mgf_parquet_dfs[table_name]\n",
    "    cmd = f\"CREATE TABLE {table_name} AS SELECT * FROM df\"\n",
    "    duckdb.sql(cmd)\n",
    "\n",
    "duckdb.sql(\"SHOW TABLES\")\n",
    "# duckdb.sql(\"COPY LSU TO 'LSU_output.csv' (HEADER, DELIMITER ',');\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6197b",
   "metadata": {},
   "source": [
    "### Create tables by superkingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "sk_dfs = {}\n",
    "for sk in [\"Eukaryota\", \"Bacteria\", \"Archaea\"]:\n",
    "    query = f\"\"\"\n",
    "            SELECT\n",
    "            *\n",
    "            FROM LSU\n",
    "            WHERE LSU.superkingdom = '{sk}'\n",
    "            \"\"\"\n",
    "    # duckdb.sql(query).show(max_width=12, max_rows=4)\n",
    "    df = duckdb.sql(query).to_df()\n",
    "    sk_dfs[sk] = df\n",
    "\n",
    "for name, df in sk_dfs.items():\n",
    "    df.drop(\"superkingdom\", axis=1, inplace=True)\n",
    "    duckdb.sql(f\"CREATE TABLE LSU_{name} AS SELECT * FROM df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT class, sum(abundance) FROM LSU_Bacteria\n",
    "WHERE class = 'Thermoanaerobaculia' \n",
    "GROUP BY class\n",
    "\"\"\"\n",
    "duckdb.sql(query).show(max_width=250, max_rows=117)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48597a5d",
   "metadata": {},
   "source": [
    "##### Testing SELECT query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54006a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = \"family\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "ref_code,\n",
    "{rank},\n",
    "sum(abundance) AS sum_abundance\n",
    "FROM LSU_Archaea\n",
    "GROUP BY {rank}, ref_code\n",
    "ORDER BY {rank}, sum(abundance) DESC\n",
    "\"\"\"\n",
    "duckdb.sql(query).show(max_width=250, max_rows=117)\n",
    "# r = duckdb.sql(query).to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095ce23-5ab6-42c2-9591-e4d17c8abc44",
   "metadata": {},
   "source": [
    "### Sum abundances per taxon in LSU per superkingdom, pivot, normalise, and take sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad2b37-166c-4ed2-9c6f-3dad9687ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to load imports and LSU table above\n",
    "\n",
    "\n",
    "def create_sqrt_normalised_table_of_rank_abundances(\n",
    "    table_name, write_tables=True, to_duckdb=False\n",
    "):\n",
    "    \"\"\"Create tables of sqrt(normalised(summed abundance)) for each taxon of each rank across all samples\"\"\"\n",
    "\n",
    "    # The rank name \"order\" interfers with ORDER being reserved word in SQL, so we need to change it\n",
    "    RANKS = [\"kingdom\", \"phylum\", \"class\", \"orderx\", \"family\", \"genus\", \"species\"]\n",
    "\n",
    "    # Check if column 'order' exists prior to changing, ie on first read\n",
    "    try:\n",
    "        duckdb.sql(f\"ALTER TABLE {table_name} RENAME COLUMN 'order' TO orderx\")\n",
    "    except BinderException as e:\n",
    "        # Exception occurs when alread\n",
    "        assert (\n",
    "            str(e)\n",
    "            == f'Binder Error: Table \"{table_name}\" does not have a column with name \"order\"'\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    dfs = {}\n",
    "    for rank in RANKS:\n",
    "        # if rank == \"phylum\" and table_name in [\"LSU_Archaea\", \"LSU_Bacteria\"]:\n",
    "        #    print(f\"Skipping {rank} for {table_name}\")\n",
    "        #    continue\n",
    "        # print(f\"{rank=}\")\n",
    "        QUERY = f\"\"\"\n",
    "        PIVOT (\n",
    "        SELECT\n",
    "        ref_code,\n",
    "        {rank},\n",
    "        sum(abundance) AS sum_abundance\n",
    "        FROM {table_name}\n",
    "        GROUP BY {rank}, ref_code\n",
    "        ORDER BY {rank}, sum(abundance) DESC   \n",
    "        )\n",
    "        ON ref_code\n",
    "        USING sum(sum_abundance)\n",
    "        ORDER BY {rank}\n",
    "        \"\"\"\n",
    "        duckdb.sql(QUERY)\n",
    "        df = duckdb.sql(QUERY).to_df()\n",
    "        df.set_index(f\"{rank}\")\n",
    "        dfs[f\"{table_name}_{rank}\"] = df\n",
    "\n",
    "    # Normalise and sqrt using loop over dataframe (can this be vectorised?)\n",
    "    def normalise_and_sqrt(dfs):\n",
    "        new_dfs = {}\n",
    "        for table_n, df in dfs.items():\n",
    "            new_df = df.copy()\n",
    "            print(f\"Normalising and converting data frame to sqrt: {table_n}\")\n",
    "            # in-place normalisation\n",
    "            for column in new_df.iloc[:, 1:].columns:\n",
    "                new_df[column] = numpy.sqrt(new_df[column] / new_df[column].abs().max())\n",
    "            new_dfs[table_n] = new_df\n",
    "        return new_dfs\n",
    "\n",
    "    ns_dfs = normalise_and_sqrt(dfs)\n",
    "\n",
    "    # Write out normalised and sqrt tables\n",
    "    if write_tables:\n",
    "        for table_n, df in ns_dfs.items():\n",
    "            save_dir = Path(\"./transformed_tables\")\n",
    "            outfile_name = f\"{table_n}.csv\"\n",
    "            df.to_csv(PurePath(save_dir, outfile_name))\n",
    "            print(f\"Written {PurePath(save_dir, outfile_name)}\")\n",
    "\n",
    "    if to_duckdb:\n",
    "        for table_n, df in ns_dfs.items():\n",
    "            duckdb.sql(f\"CREATE TABLE {table_n} AS SELECT * FROM df\")\n",
    "            print(f\"Created {table_n} in duckdb\")\n",
    "\n",
    "\n",
    "# For each superkingdom table, create a table of sqrt(normalised(summed abundance))\n",
    "for table_name in [\"LSU_Archaea\", \"LSU_Eukaryota\", \"LSU_Bacteria\"]:\n",
    "    create_sqrt_normalised_table_of_rank_abundances(\n",
    "        table_name, write_tables=True, to_duckdb=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
